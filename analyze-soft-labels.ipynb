{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import skweak\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import ast\n",
    "import torch\n",
    "import gc\n",
    "import shutil\n",
    "import evaluate\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tokenizations\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.nn import functional\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import myutils\n",
    "import aggregation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the data\n",
    "path_data = 'data/'\n",
    "file_name = 'training_ws_lfs.json'\n",
    "rubrics = myutils.load_rubrics(path_data + 'rubrics.json')\n",
    "annotated_train_data = read_from_json(path_data + file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate stats splitted by German and English\n",
    "stats_de = {}\n",
    "stats_en = {}\n",
    "# initialize stats dict  for the individual labeling_functions\n",
    "for k in annotated_train_data[0]['labeling_functions']:\n",
    "    stats_de[k] ={'class':[], 'labels':[]}\n",
    "for k in annotated_train_data[0]['labeling_functions']:\n",
    "    stats_en[k] ={'class':[], 'labels':[]}\n",
    "\n",
    "for an in annotated_train_data:\n",
    "    c = an['label']\n",
    "    lang = an['lang']\n",
    "    if lang == 'en':\n",
    "        for k,v in an['labeling_functions'].items():\n",
    "            stats_en[k]['labels'].append(v)\n",
    "            stats_en[k]['class'].append(c)\n",
    "    elif lang == 'de':\n",
    "         for k,v in an['labeling_functions'].items():\n",
    "            stats_de[k]['labels'].append(v)\n",
    "            stats_de[k]['class'].append(c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ANALYSIS START"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp_de = spacy.load(\"de_core_news_lg\")\n",
    "german_question_ids = [str(i) for i in range(1,9)]\n",
    "th = 0.5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyze individual LFs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Distribution of labels per LF\n",
    "for lang, stats in {'DE': stats_de, 'EN': stats_en}.items():\n",
    "    bin_items = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,0.7,0.8,0.9,1.0]\n",
    "    for k,v in stats.items():\n",
    "        correct, partial_correct, incorrect = [],[],[]\n",
    "        for c, l in zip(v['class'], v['labels']):\n",
    "            if c == 'CORRECT':\n",
    "                correct.append(l)\n",
    "            elif c == 'PARTIAL_CORRECT':\n",
    "                partial_correct.append(l)\n",
    "            elif c == 'INCORRECT':\n",
    "                incorrect.append(l)\n",
    "        plt.title(lang + ' ' + k)\n",
    "        plt.xlabel('Data')\n",
    "        plt.ylabel('Frequency')\n",
    "        correct = flat_list(correct)\n",
    "        correct = [i for i in correct if i != 0.0]\n",
    "        partial_correct = flat_list(partial_correct)\n",
    "        partial_correct = [i for i in partial_correct if i != 0.0]\n",
    "        incorrect = flat_list(incorrect)\n",
    "        incorrect = [i for i in incorrect if i != 0.0]\n",
    "        plt.hist([correct, partial_correct, incorrect], label=['CORRECT', 'PARTIAL_CORRECT', 'INCORRECT'])\n",
    "        plt.xticks(bin_items)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Share of as relevant labeled tokens based on a threshold (>=0.5)\n",
    "for lang, stats in {'DE': stats_de, 'EN': stats_en}.items():\n",
    "   \n",
    "    for k,v in stats.items():\n",
    "        relevants, unrelevants = [],[]\n",
    "        for labels in v['labels']:\n",
    "            relevant, unrelevant = 0,0\n",
    "            for label in labels:\n",
    "                for l in labels:\n",
    "                    if l > th:\n",
    "                        relevant+=1\n",
    "                    else:\n",
    "                        unrelevant+=1\n",
    "            relevants.append(relevant)\n",
    "            unrelevants.append(unrelevant)\n",
    "        \n",
    "        plt.title(lang + ' ' + k)\n",
    "        plt.pie([np.sum(relevants), np.sum(unrelevants)], labels = ['relevant_token', 'unrelevant_token'],autopct='%1.1f%%')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    print(20*'-')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Share of the general labeled tokens per LF\n",
    "for lang, stats in {'DE': stats_de, 'EN': stats_en}.items():\n",
    "   \n",
    "    for k,v in stats.items():\n",
    "        relevants, unrelevants = [],[]\n",
    "        for labels in v['labels']:\n",
    "            relevant, unrelevant = 0,0\n",
    "            for label in labels:\n",
    "                for l in labels:\n",
    "                    if l > 0.0:\n",
    "                        relevant+=1\n",
    "                    else:\n",
    "                        unrelevant+=1\n",
    "            relevants.append(relevant)\n",
    "            unrelevants.append(unrelevant)\n",
    "        \n",
    "        plt.title(lang + ' ' + k)\n",
    "        plt.pie([np.sum(relevants), np.sum(unrelevants)], labels = ['labeled_token', 'unlabeled_token'],autopct='%1.1f%%')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    print(20*'-')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# labeled tokens per LF per class\n",
    "for lang, stats in {'DE': stats_de, 'EN': stats_en}.items():\n",
    "    data = {'CORRECT': [],'PARTIAL_CORRECT': [], 'INCORRECT': []}\n",
    "    lfs = list(stats.keys())\n",
    "    for k,v in stats.items():\n",
    "        correct, partial_correct, incorrect = [],[],[]\n",
    "        for c, l in zip(v['class'], v['labels']):\n",
    "            if c == 'CORRECT':\n",
    "                correct.append(l)\n",
    "            elif c == 'PARTIAL_CORRECT':\n",
    "                partial_correct.append(l)\n",
    "            elif c == 'INCORRECT':\n",
    "                incorrect.append(l)\n",
    "        # flatten and check if relevant -> <= 0.5\n",
    "        correct = flat_list(correct)\n",
    "        correct = len([i for i in correct if i >= th])\n",
    "        partial_correct = flat_list(partial_correct)\n",
    "        partial_correct = len([i for i in partial_correct if i >= th])\n",
    "        incorrect = flat_list(incorrect)\n",
    "        incorrect = len([i for i in incorrect if i > th])\n",
    "        data['CORRECT'].append(correct)\n",
    "        data['PARTIAL_CORRECT'].append(partial_correct)\n",
    "        data['INCORRECT'].append(incorrect)\n",
    "    \n",
    "    plt.title(lang + ' Number of relevant token per LF')\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('Frequency')    \n",
    "    plt.plot(lfs, data['CORRECT'] ,label='CORRECT')\n",
    "    plt.plot(lfs, data['PARTIAL_CORRECT'] ,label='PARTIAL_CORRECT')\n",
    "    plt.plot(lfs, data['INCORRECT'] ,label='INCORRECT')\n",
    "    plt.xticks(lfs, rotation=90)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# How long and how many key elements does a LF detect?\n",
    "for lang, stats in {'DE': stats_de, 'EN': stats_en}.items():\n",
    "    data = {'CORRECT': {'avg_relation': [], 'avg_len_rubrics': [], 'avg_token_per_element': []},'PARTIAL_CORRECT': {'avg_relation': [], 'avg_len_rubrics': [], 'avg_token_per_element': []}, 'INCORRECT': {'avg_relation': [], 'avg_len_rubrics': [], 'avg_token_per_element': []}}\n",
    "    lfs = list(stats.keys())\n",
    "    for k,v in stats.items():\n",
    "        correct, partial_correct, incorrect = [],[],[]\n",
    "        for c, l in zip(v['class'], v['labels']):\n",
    "            if c == 'CORRECT':\n",
    "                correct.append(l)\n",
    "            elif c == 'PARTIAL_CORRECT':\n",
    "                partial_correct.append(l)\n",
    "            elif c == 'INCORRECT':\n",
    "                incorrect.append(l)\n",
    "        correct = [silver2target(labels, th) for labels in correct]\n",
    "        #correct = [i for i in correct if i >= th]\n",
    "        partial_correct = [silver2target(labels, th) for labels in partial_correct]\n",
    "        #partial_correct = len([i for i in partial_correct if i >= th])\n",
    "        incorrect = [silver2target(labels, th) for labels in incorrect]\n",
    "        \n",
    "        # dynamic values for every LF\n",
    "        relations, len_rubrics, num_token_per_element = [],[], []\n",
    "        for i,c in enumerate([correct, partial_correct, incorrect]):\n",
    "            rels = [relation(l) for l in c]\n",
    "            len_rubric = [len(rubric_length(l)) for l in c]\n",
    "            token_per_element = [np.average(rubric_length(l)) for l in c]\n",
    "            token_per_element = [t for t in token_per_element if not np.isnan(t)]\n",
    "            avg_relations = np.average(rels)\n",
    "            avg_len_rubrics = np.average(len_rubric)\n",
    "            if len(token_per_element) == 0:\n",
    "                token_per_element.append(0)\n",
    "            avg_token_per_element = np.average(token_per_element)\n",
    "            #relations.append(avg_relations)\n",
    "            #len_rubrics.append(avg_len_rubrics)\n",
    "            #num_token_per_element.append(avg_len_rubrics)\n",
    "            if i == 0: \n",
    "                l = 'CORRECT'\n",
    "            elif i == 1: \n",
    "                l = 'PARTIAL_CORRECT'\n",
    "            elif i == 2: \n",
    "                l = 'INCORRECT'\n",
    "            data[l]['avg_relation'].append(avg_relations)\n",
    "            data[l]['avg_len_rubrics'].append(avg_len_rubrics) \n",
    "            data[l]['avg_token_per_element'].append(avg_token_per_element) \n",
    "    \n",
    "    # TODO adapt it here\n",
    "    plt.xlabel('LFs')\n",
    "    #plt.ylabel() \n",
    "    for l in ['avg_relation', 'avg_len_rubrics', 'avg_token_per_element']:\n",
    "        plt.title(lang + ' Stats ' + l)\n",
    "        plt.plot(lfs, data['CORRECT'][l] ,label='CORRECT')\n",
    "        plt.plot(lfs, data['PARTIAL_CORRECT'][l] ,label='PARTIAL_CORRECT')\n",
    "        plt.plot(lfs, data['INCORRECT'][l],label='INCORRECT')\n",
    "        plt.legend()\n",
    "        plt.xticks(lfs, rotation=90)\n",
    "        plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Aggregation functions\n",
    "Analyze how the different aggregation techniches distribute the labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "GLOBAL_NORMALIZE = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# different aggregation functions\n",
    "# exclude all hard match functions\n",
    "#exclude = ['LF_word_alginment', 'LF_dep_match_without_stopwords', 'LF_dep_match', 'LF_tag_match', 'LF_stem_match', 'LF_pos_match_without_stopwords', 'LF_lemma_match_without_stopwords','LF_pos_match' , 'LF_pos_match', 'LF_noun_phrase']\n",
    "exclude=[]\n",
    "#exclude = ['LF_dep_match_without_stopwords', 'LF_dep_match', 'LF_tag_match','LF_pos_match_without_stopwords', 'LF_lemma_match_without_stopwords', 'LF_pos_match', 'LF_noun_phrases' ]\n",
    "#exclude = ['LF_parahrase_detection_candidates', 'LF_parahrase_detection_sentences', 'LF_bleu_candidates', 'LF_jaccard_similarity', 'LF_edit_distance']\n",
    "annotations_train = extract_annotations(annotated_train_data, exclude_LFs=exclude)\n",
    "\n",
    "#annotations_train = extract_annotations_only_LFs(annotated_train_data, lfs=['LF_parahrase_detection_sentences', 'LF_parahrase_detection_candidates'])\n",
    "#annotations_dev = extract_annotations_only_LFs(annotated_dev_data, lfs=['LF_parahrase_detection_sentences', 'LF_parahrase_detection_candidates'])\n",
    "\n",
    "def global_normalize(data):\n",
    "    # finding min and max value\n",
    "    min_value = 100\n",
    "    max_value = -100\n",
    "    max_vals = []\n",
    "    min_vals = []\n",
    "    for d in data:\n",
    "        v_min = np.min(d)\n",
    "        v_max = np.max(d)\n",
    "        if v_min < min_value:\n",
    "            min_value = v_min\n",
    "            min_vals.append(min_value)\n",
    "        if v_max > max_value:\n",
    "            max_value = v_max\n",
    "            max_vals.append(max_value)\n",
    "    print('max', max_vals, 'min', min_vals)\n",
    "    range_val = max_value - min_value\n",
    "    data_norm = [[(l - min_value) / range_val for l in d] for d in data]\n",
    "    return data_norm\n",
    "\n",
    "for lang in ['de', 'en']:\n",
    "    for mode in ['average', 'max', 'average_nonzero', 'sum']:\n",
    "        stats = {\n",
    "            'CORRECT':[],\n",
    "            'PARTIAL_CORRECT': [],\n",
    "            'INCORRECT': []\n",
    "        }\n",
    "        labels = []\n",
    "        for a, an in zip(annotations_train, annotated_train_data):\n",
    "            y = aggregate_soft_labels(a, mode)\n",
    "            if not GLOBAL_NORMALIZE:\n",
    "                y = normalize(y)\n",
    "            labels.append(y)\n",
    "        if GLOBAL_NORMALIZE:\n",
    "            labels = global_normalize(labels)\n",
    "        print(mode)\n",
    "        for an, label in zip(annotated_train_data, labels):\n",
    "            c = an['label']\n",
    "            l = an['lang']\n",
    "            if l  == lang:\n",
    "                #for mode in ['average', 'max', 'average_nonzero', 'sum']:\n",
    "                stats[c].append(label)\n",
    "        \n",
    "        bin_items = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "        #counts, bins = np.histogram(x,bins=bin_items)\n",
    "        plt.title(lang.upper() + ' ' + mode)\n",
    "        plt.xlabel('Data')\n",
    "        plt.ylabel('Frequency')\n",
    "        correct = flat_list(stats['CORRECT'])\n",
    "        partial_correct = flat_list(stats['PARTIAL_CORRECT'])\n",
    "        incorrect = flat_list(stats['INCORRECT'])\n",
    "        normalized_data = []\n",
    "        weights = []\n",
    "        \n",
    "        for d in [correct, partial_correct, incorrect]:\n",
    "            w = np.ones_like(d) / len(d)\n",
    "            weights.append(w)\n",
    "        plt.hist([correct, partial_correct, incorrect], bins=bin_items, weights=weights, label=['CORRECT', 'PARTIAL_CORRECT', 'INCORRECT'])\n",
    "        plt.xticks(bin_items)\n",
    "        plt.legend()\n",
    "            #n, bins = np.histogram(d, bins=bin_items,density=True)\n",
    "            #n = n / n.sum()\n",
    "            #print(n)\n",
    "            #normalized_data.append(n)\n",
    "            #plt.bar(normalized_data, bins=bin_items)\n",
    "        #plt.hist(normalized_data[1],ins=bin_items)\n",
    "        #plt.hist(normalized_data[2], ins=bin_items)\n",
    "       \n",
    "        #data = np.concatenate([correct, partial_correct, incorrect])\n",
    "        #n, bins = np.histogram(data, bins=bin_items,density=True)\n",
    "        # Normalize the histogram\n",
    "        #n = n / n.sum()\n",
    "        #print(n)\n",
    "        #plt.hist(n, bins=bin_items, )\n",
    "        # Plot the normalized histogram\n",
    "        #plt.bar(bin_items, n[0], width=0.05)\n",
    "        #plt.bar(bin_items, n[1], width=0.05)\n",
    "        #plt.bar(bin_items, n[2], width=0.05)\n",
    "\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# different aggregation functions\n",
    "# exclude all hard match functions\n",
    "#exclude = ['LF_word_alginment', 'LF_dep_match_without_stopwords', 'LF_dep_match', 'LF_tag_match', 'LF_stem_match', 'LF_shape_match', 'LF_pos_match_without_stopwords', 'LF_lemma_match_without_stopwords', 'LF_pos_match', 'LF_lemma_match', 'LF_noun_phrase']\n",
    "exclude=[]\n",
    "#exclude = ['LF_dep_match_without_stopwords', 'LF_dep_match', 'LF_tag_match','LF_pos_match_without_stopwords', 'LF_lemma_match_without_stopwords', 'LF_pos_match', 'LF_noun_phrases' ]\n",
    "#exclude = ['LF_parahrase_detection_candidates', 'LF_parahrase_detection_sentences', 'LF_bleu_candidates', 'LF_jaccard_similarity', 'LF_edit_distance']\n",
    "annotations_train = extract_annotations(annotated_train_data, exclude_LFs=exclude)\n",
    "#annotations_train = extract_annotations_only_LFs(annotated_train_data, lfs=['LF_parahrase_detection_sentences', 'LF_parahrase_detection_candidates'])\n",
    "#annotations_dev = extract_annotations_only_LFs(annotated_dev_data, lfs=['LF_parahrase_detection_sentences', 'LF_parahrase_detection_candidates'])\n",
    "\n",
    "def global_normalize(data):\n",
    "    # finding min and max value\n",
    "    min_value = 100\n",
    "    max_value = -100\n",
    "    max_vals = []\n",
    "    min_vals = []\n",
    "    for d in data:\n",
    "        v_min = np.min(d)\n",
    "        v_max = np.max(d)\n",
    "        if v_min < min_value:\n",
    "            min_value = v_min\n",
    "            min_vals.append(min_value)\n",
    "        if v_max > max_value:\n",
    "            max_value = v_max\n",
    "            max_vals.append(max_value)\n",
    "    print('max', max_vals, 'min', min_vals)\n",
    "    #min_value = min_vals[0]\n",
    "    #max_value = max_vals[0]\n",
    "    #min_value = np.average(min_vals)\n",
    "    #max_value = np.average(max_vals)\n",
    "    range_val = max_value - min_value\n",
    "    data_norm = [[(l - min_value) / range_val for l in d] for d in data]\n",
    "    return data_norm\n",
    "\n",
    "for mode in ['average', 'max', 'average_nonzero', 'sum']:\n",
    "        stats = {\n",
    "            'CORRECT':[],\n",
    "            'PARTIAL_CORRECT': [],\n",
    "            'INCORRECT': []\n",
    "        }\n",
    "        labels = []\n",
    "        for a, an in zip(annotations_train, annotated_train_data):\n",
    "            y = aggregate_soft_labels(a, mode)\n",
    "            y= normalize(y)\n",
    "            labels.append(y)\n",
    "        #print(labels[0])\n",
    "        #if mode == 'sum':\n",
    "        print(mode)\n",
    "        #labels = global_normalize(labels)\n",
    "        #print(labels[0])\n",
    "        for an, label in zip(annotated_train_data, labels):\n",
    "            c = an['label']\n",
    "            \n",
    "            #for mode in ['average', 'max', 'average_nonzero', 'sum']:\n",
    "            stats[c].append(label)\n",
    "        \n",
    "        bin_items = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "        #counts, bins = np.histogram(x,bins=bin_items)\n",
    "        plt.title(mode)\n",
    "        plt.xlabel('Data')\n",
    "        plt.ylabel('Frequency')\n",
    "        correct = flat_list(stats['CORRECT'])\n",
    "        partial_correct = flat_list(stats['PARTIAL_CORRECT'])\n",
    "        incorrect = flat_list(stats['INCORRECT'])\n",
    "        normalized_data = []\n",
    "        weights = []\n",
    "        \n",
    "        for d in [correct, partial_correct, incorrect]:\n",
    "            w = np.ones_like(d) / len(d)\n",
    "            weights.append(w)\n",
    "        plt.hist([correct, partial_correct, incorrect], bins=bin_items, weights=weights, label=['CORRECT', 'PARTIAL_CORRECT', 'INCORRECT'])\n",
    "        plt.xticks(bin_items)\n",
    "        plt.legend()\n",
    "            #n, bins = np.histogram(d, bins=bin_items,density=True)\n",
    "            #n = n / n.sum()\n",
    "            #print(n)\n",
    "            #normalized_data.append(n)\n",
    "            #plt.bar(normalized_data, bins=bin_items)\n",
    "        #plt.hist(normalized_data[1],ins=bin_items)\n",
    "        #plt.hist(normalized_data[2], ins=bin_items)\n",
    "       \n",
    "        #data = np.concatenate([correct, partial_correct, incorrect])\n",
    "        #n, bins = np.histogram(data, bins=bin_items,density=True)\n",
    "        # Normalize the histogram\n",
    "        #n = n / n.sum()\n",
    "        #print(n)\n",
    "        #plt.hist(n, bins=bin_items, )\n",
    "        # Plot the normalized histogram\n",
    "        #plt.bar(bin_items, n[0], width=0.05)\n",
    "        #plt.bar(bin_items, n[1], width=0.05)\n",
    "        #plt.bar(bin_items, n[2], width=0.05)\n",
    "\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyze Rubric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "german_ids = [str(i) for i in range(1,10)]\n",
    "\n",
    "x_de= []\n",
    "x_en= []\n",
    "for k in rubrics.keys():\n",
    "    if k in german_ids:\n",
    "        x_de.append(len(rubrics[k]['key_element']))\n",
    "    else:\n",
    "        x_en.append(len(rubrics[k]['key_element']))\n",
    "\n",
    "print('Average key elements DE', np.average(x_de))\n",
    "print('Average key elements EN', np.average(x_en))\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Occurrences')\n",
    "plt.title('Key elements in rubrics')\n",
    "plt.hist([x_de, x_en], label=['DE', 'EN'])\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyze Annotated Key Elements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for language in ['de', 'en']:\n",
    "    relations_correct, relations_partial, relations_incorrect = [], [],[]\n",
    "    len_rubrics_correct, len_rubrics_partial, len_rubrics_incorrect = [], [],[]\n",
    "    len_rubrics_average_correct, len_rubrics_average_partial, len_rubrics_average_incorrect = [], [],[]\n",
    "    for an, labels in zip(annotated_train_data, silver_labels):\n",
    "        lang = an['lang']\n",
    "        c = an['label']\n",
    "        if lang == language:\n",
    "            true_labels = silver2target(labels,th=th)\n",
    "            rel = relation(true_labels)\n",
    "            len_rubrics = rubric_length(true_labels)\n",
    "            if len(len_rubrics) > 0:\n",
    "                len_rubrics_average = np.average(len_rubrics)\n",
    "            else:\n",
    "                len_rubrics_average = 0\n",
    "            if c == 'CORRECT':\n",
    "                relations_correct.append(rel)\n",
    "                len_rubrics_correct.append(len(len_rubrics))\n",
    "                len_rubrics_average_correct.append(len_rubrics_average)\n",
    "            elif c == 'PARTIAL_CORRECT':\n",
    "                relations_partial.append(rel)\n",
    "                len_rubrics_partial.append(len(len_rubrics))\n",
    "                len_rubrics_average_partial.append(len_rubrics_average)\n",
    "            elif c == 'INCORRECT':\n",
    "                relations_incorrect.append(rel)\n",
    "                len_rubrics_incorrect.append(len(len_rubrics))\n",
    "                len_rubrics_average_incorrect.append(len_rubrics_average)\n",
    "    \n",
    "    print(language.upper() +' Relation:','CORRECT', np.average(relations_correct), 'PARTIAL_CORRECT', np.average(relations_partial),'INCORRECT', np.average(relations_incorrect))\n",
    "    \n",
    "    plot_hist({'CORRECT': len_rubrics_average_correct, 'PARTIAL_CORRECT': len_rubrics_average_partial, 'INCORRECT': len_rubrics_average_incorrect}, bins=range(0,50,5), title=language.upper()+' Number of tokens in key element')         \n",
    "    print(language.upper() + ' Average len (tokens):', 'CORRECT',np.average(len_rubrics_average_correct),'PARTIAL_CORRECT', np.average(len_rubrics_average_partial), 'INCORRECT', np.average(len_rubrics_average_incorrect))\n",
    "    \n",
    "    plot_hist({'CORRECT': len_rubrics_correct, 'PARTIAL_CORRECT': len_rubrics_partial, 'INCORRECT': len_rubrics_incorrect}, bins=range(0,20,2), title=language.upper() + ' Number of key elements in answer')         \n",
    "    print(language.upper() + ' Average number of rubrics in answer:', 'CORRECT',np.average(len_rubrics_correct), 'PARTIAL_CORRECT',np.average(len_rubrics_partial),'INCORRECT', np.average(len_rubrics_incorrect))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question specific data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for question_id in np.unique(list(rubrics.keys())):\n",
    "    relations_correct, relations_partial, relations_incorrect = [], [],[]\n",
    "    len_rubrics_correct, len_rubrics_partial, len_rubrics_incorrect = [], [],[]\n",
    "    len_rubrics_average_correct, len_rubrics_average_partial, len_rubrics_average_incorrect = [], [],[]\n",
    "    for an, labels in zip(annotated_train_data, silver_labels):\n",
    "        q_id = an['question_id']\n",
    "        c = an['label']\n",
    "        if q_id == question_id:\n",
    "            true_labels = silver2target(labels)\n",
    "            rel = relation(true_labels)\n",
    "            len_rubrics = rubric_length(true_labels)\n",
    "            if len(len_rubrics) > 0:\n",
    "                len_rubrics_average = np.average(len_rubrics)\n",
    "            else:\n",
    "                len_rubrics_average = 0\n",
    "            if c == 'CORRECT':\n",
    "                relations_correct.append(rel)\n",
    "                len_rubrics_correct.append(len(len_rubrics))\n",
    "                len_rubrics_average_correct.append(len_rubrics_average)\n",
    "            elif c == 'PARTIAL_CORRECT':\n",
    "                relations_partial.append(rel)\n",
    "                len_rubrics_partial.append(len(len_rubrics))\n",
    "                len_rubrics_average_partial.append(len_rubrics_average)\n",
    "            elif c == 'INCORRECT':\n",
    "                relations_incorrect.append(rel)\n",
    "                len_rubrics_incorrect.append(len(len_rubrics))\n",
    "                len_rubrics_average_incorrect.append(len_rubrics_average)\n",
    "    \n",
    "    print(question_id +' Relation:','CORRECT', np.average(relations_correct), 'PARTIAL_CORRECT', np.average(relations_partial),'INCORRECT', np.average(relations_incorrect))\n",
    "    \n",
    "    plot_hist({'CORRECT': len_rubrics_average_correct, 'PARTIAL_CORRECT': len_rubrics_average_partial, 'INCORRECT': len_rubrics_average_incorrect}, bins=range(0,50,5), title=question_id+' Number of tokens in key element')         \n",
    "    print(question_id + ' Average len (tokens):', 'CORRECT',np.average(len_rubrics_average_correct),'PARTIAL_CORRECT', np.average(len_rubrics_average_partial), 'INCORRECT', np.average(len_rubrics_average_incorrect))\n",
    "    \n",
    "    plot_hist({'CORRECT': len_rubrics_correct, 'PARTIAL_CORRECT': len_rubrics_partial, 'INCORRECT': len_rubrics_incorrect}, bins=range(0,20,2), title=question_id + ' Number of key elements in answer')         \n",
    "    print(question_id+ ' Average number of rubrics in answer:', 'CORRECT',np.average(len_rubrics_correct), 'PARTIAL_CORRECT',np.average(len_rubrics_partial),'INCORRECT', np.average(len_rubrics_incorrect))\n",
    "    print(30*'-' + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}